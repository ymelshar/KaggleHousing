{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c91dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic inputs for most ML tasks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import tree\n",
    "# import graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "# setup interactive notebook mode\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206caab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fetch data \n",
    "train_data = pd.read_csv('Datasets/train.csv')\n",
    "test_data = pd.read_csv('Datasets/test.csv')\n",
    "\n",
    "# display first few rows of train data\n",
    "train_data.head()\n",
    "test_data.head()\n",
    "\n",
    "# length of train data\n",
    "len(train_data)\n",
    "len(test_data)\n",
    "\n",
    "# sum of NaN values\n",
    "train_data.isna().sum()\n",
    "test_data.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d84a2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dropPoolQC and MiscFeature due to high amount of NaN values within column (>3/4 of data length)\n",
    "\n",
    "train_data.drop(columns = ['PoolQC','MiscFeature'], inplace = True)\n",
    "test_data.drop(columns = ['PoolQC', 'MiscFeature'], inplace = True)\n",
    "\n",
    "# Drop Id as it doesn't do anything for the data\n",
    "train_data.drop(['Id'], axis=1, inplace=True)\n",
    "test_data.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "# view SalePrice distribution\n",
    "train_data['SalePrice'].hist(bins = 40)\n",
    "\n",
    "# looks like SalePrice is skewed, so let's fix that\n",
    "train_data = train_data[train_data.GrLivArea < 4500]\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\n",
    "\n",
    "# keep our SalePrice column as our dependent variable\n",
    "y_train = train_data['SalePrice'].reset_index(drop=True)\n",
    "\n",
    "# now it's more like a normal distribution\n",
    "train_data['SalePrice'].hist(bins = 40)\n",
    "\n",
    "# combine both train and test data to handle NaNs and missing values more easily\n",
    "train_features = train_data.drop(['SalePrice'], axis=1)\n",
    "test_features = test_data\n",
    "features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
    "\n",
    "# Since these column are actually a category , using a numerical number will lead the model to assume\n",
    "# that it is numerical , so we convert to string .\n",
    "features['MSSubClass'] = features['MSSubClass'].apply(str)\n",
    "features['YrSold'] = features['YrSold'].astype(str)\n",
    "features['MoSold'] = features['MoSold'].astype(str)\n",
    "\n",
    "## Filling these columns With most suitable value for these columns \n",
    "features['Functional'] = features['Functional'].fillna('Typ') \n",
    "features['Electrical'] = features['Electrical'].fillna(\"SBrkr\") \n",
    "features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\") \n",
    "\n",
    "## Filling these with MODE , i.e. , the most frequent value in these columns .\n",
    "features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0]) \n",
    "features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n",
    "features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n",
    "features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "# fill garage data\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    features[col] = features[col].fillna(0)\n",
    "\n",
    "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    features[col] = features[col].fillna('None')\n",
    "    \n",
    "# Fill basement data\n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    features[col] = features[col].fillna('None')\n",
    "    \n",
    "# Fill rest of object/categorical features with None\n",
    "objects = []\n",
    "for i in features.columns:\n",
    "    if features[i].dtype == object:\n",
    "        objects.append(i)\n",
    "features.update(features[objects].fillna('None'))\n",
    "\n",
    "# Fill rest of numerical features with 0\n",
    "numeric_dtypes = ['int32', 'int64', 'float32', 'float64']\n",
    "numerics = []\n",
    "for i in features.columns:\n",
    "    if ((features[i].dtype in numeric_dtypes) & ~(features[i].equals(features['LotFrontage']))) :\n",
    "        numerics.append(i)\n",
    "features.update(features[numerics].fillna(0))\n",
    "\n",
    "# treat the skewed data through boxcox transformation\n",
    "# numerics2 = []\n",
    "# for i in features.columns:\n",
    "#     if features[i].dtype in numeric_dtypes:\n",
    "#         numerics2.append(i)\n",
    "# skew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "# high_skew = skew_features[skew_features > 0.5]\n",
    "# skew_index = high_skew.index\n",
    "\n",
    "# for i in skew_index:\n",
    "#    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n",
    "\n",
    "len(train_data)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING PORTION\n",
    "# Removing features that are not very useful . Logically, Utilities and Street shouldn't contribute much to SalePrice\n",
    "\n",
    "features = features.drop(['Utilities', 'Street'], axis=1)\n",
    "\n",
    "\n",
    "# Adding new features to condense the data\n",
    "\n",
    "features['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\n",
    "features['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n",
    "\n",
    "features['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n",
    "                                 features['1stFlrSF'] + features['2ndFlrSF'])\n",
    "\n",
    "features['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n",
    "                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n",
    "\n",
    "features['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n",
    "                              features['EnclosedPorch'] + features['ScreenPorch'] +\n",
    "                              features['WoodDeckSF'])\n",
    "\n",
    "features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3298f7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "# From observing, it will be for the best to fill in LotFrontage NaNs with some values\n",
    "# To do this, we will use features that seem to have a correlation with LotFrontage in order to train\n",
    "# and predict the values for those that have a NaN value initially.\n",
    "\n",
    "# For reference and citing outside sources, I will be referring to \n",
    "# https://www.kaggle.com/code/ogakulov/lotfrontage-fill-in-missing-values-house-prices\n",
    "# as a source for methods and choosing what feature variables to use in order to predict LotFrontage.\n",
    "\n",
    "# Instead of using the SVR classifier as the article does, I will attempt to use gradient boosting regressor.\n",
    "\n",
    "# Drop SalePrice column from train dataset and merge into one data frame called all_data\n",
    "training_data = train_data.drop('SalePrice', axis=1)\n",
    "testing_data = test_data\n",
    "all_data = pd.concat([training_data, testing_data], ignore_index=True).copy()\n",
    "\n",
    "# Split into known and unknown LotFrontage records\n",
    "lotFrontage_test = features[features.LotFrontage.isnull()]\n",
    "lotFrontage_train = features[~features.LotFrontage.isnull()]\n",
    "target = lotFrontage_train.LotFrontage\n",
    "print(\"LotFrontage has {:} missing value, and {:} values avaialble.\".format(lotFrontage_test.shape[0], lotFrontage_train.shape[0]))\n",
    "\n",
    "# Pull only the features for training the model. Define target variable\n",
    "y_lotFrontage_train = lotFrontage_train['LotFrontage']\n",
    "x_lotFrontage_train = lotFrontage_train.loc[:,['LotArea', 'LotConfig', 'LotShape', 'MSZoning', 'BldgType', 'Neighborhood', 'Condition1', 'Condition2', 'GarageCars']]\n",
    "\n",
    "# Dummify categorical variables and normalize the data\n",
    "x_lotFrontage_train = pd.get_dummies(x_lotFrontage_train)\n",
    "x_lotFrontage_train = (x_lotFrontage_train - x_lotFrontage_train.mean())/x_lotFrontage_train.std()\n",
    "x_lotFrontage_train = x_lotFrontage_train.fillna(0)\n",
    "\n",
    "# From Assignment 4,from testing which parameters would have given the minimum sMAPE, I decided to replicate \n",
    "# those same parameters for gradient boosting\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate = 0.3, max_depth=11)\n",
    "# gb = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state = 69)\n",
    "# gb = XGBRegressor(learning_rate=0.11321366170467694, max_depth=3, n_estimators=832, subsample=0.8126543182197247, colsample_bytree=0.8424884692926351)\n",
    "gb.fit(x_lotFrontage_train, y_lotFrontage_train)\n",
    "\n",
    "gb.score(x_lotFrontage_train, y_lotFrontage_train)\n",
    "\n",
    "# use gradient boosting to fill in NaN values through prediction\n",
    "\n",
    "# Select columns for final prediction, dummify, and normalize\n",
    "features_lotFrontage_NaN = features[features.LotFrontage.isnull()]\n",
    "features_lotFrontage = features_lotFrontage_NaN.loc[:,['LotArea', 'LotConfig', 'LotShape', 'MSZoning', 'BldgType', 'Neighborhood', 'Condition1', 'Condition2', 'GarageCars']]\n",
    "features_lotFrontage = pd.get_dummies(features_lotFrontage)\n",
    "features_lotFrontage = (features_lotFrontage - features_lotFrontage.mean())/features_lotFrontage.std()\n",
    "features_lotFrontage = features_lotFrontage.fillna(0)\n",
    "\n",
    "# Make sure that dummy columns from training set are replicated in test set\n",
    "for col in (set(x_lotFrontage_train.columns) - set(features_lotFrontage.columns)):\n",
    "    features_lotFrontage[col] = 0\n",
    "\n",
    "features_lotFrontage = features_lotFrontage[x_lotFrontage_train.columns]\n",
    "\n",
    "# Assign predicted LotFrontage value into train_data \n",
    "features.loc[features.LotFrontage.isnull(), 'LotFrontage'] = gb.predict(features_lotFrontage)\n",
    "\n",
    "features.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7908c6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use OneHotEncoder instead of getdummies\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Identify categorical columns\n",
    "features_col = features.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Extract categorical columns\n",
    "features_cat = features[features_col]\n",
    "\n",
    "# Using OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "features_encoded = pd.DataFrame(encoder.fit_transform(features_cat), columns=encoder.get_feature_names_out(features_col))\n",
    "\n",
    "# Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
    "features = pd.concat([features, features_encoded], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "features = features.drop(features_col, axis=1)\n",
    "\n",
    "features.isna().sum().sum()\n",
    "features.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  split train and test are back seperately \n",
    "if True:\n",
    "    X_train = features.iloc[:len(y_train), :]\n",
    "    X_test = features.iloc[len(y_train):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "if True:\n",
    "    #Feature Scaling\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, index = X_train.index, columns = X_train.columns)\n",
    "    X_test_scaled = scaler.transform(X_test.values)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, index = X_test.index, columns = X_test.columns)\n",
    "    X_train = X_train_scaled_df\n",
    "    X_test = X_test_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c36943",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    from mlxtend.regressor import StackingCVRegressor\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RationalQuadratic, Exponentiation\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.linear_model import LassoCV\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "    from sklearn.preprocessing import RobustScaler, normalize\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # defining error functions for handy use. \n",
    "\n",
    "    kfolds = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "\n",
    "    def rmsle(y, y_pred):\n",
    "        return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "    def cv_rmse(model, X_train=X_train):\n",
    "        rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "        return (rmse)\n",
    "    \n",
    "    ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=[1e-06, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], \n",
    "                                                  cv=kfolds))\n",
    "    lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1000000, cv=kfolds, random_state=7,\n",
    "                                        alphas=[1e-06, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]))\n",
    "    gb_regress = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', \n",
    "                                           min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=7)\n",
    "    \n",
    "     # Define the hyperparameter space\n",
    "    lgb_space = {\n",
    "        'max_depth': 5, \n",
    "        'learning_rate': 0.15, \n",
    "        'n_estimators': 3800, \n",
    "        'num_leaves': 4,\n",
    "        'max_bin': 150,\n",
    "        'bagging_fraction': 0.7434141086967856, \n",
    "        'bagging_freq': 14,\n",
    "        'objective': 'regression',\n",
    "        'tree_learner': 'feature',\n",
    "        'boosting_type' : 'dart',\n",
    "        'verbosity': -1,\n",
    "        'random_state': 7\n",
    "    }\n",
    "    \n",
    "    lightgb_regress = LGBMRegressor(**lgb_space)\n",
    "    \n",
    "    \n",
    "    # Define the hyperparameter space\n",
    "    \n",
    "    xgb_space = {\n",
    "        'max_depth': 3, \n",
    "        'max_leaves': 24, \n",
    "        'learning_rate': 0.008073243888388325, \n",
    "        'n_estimators': 5300,\n",
    "        'subsample': 0.6982127263866671,\n",
    "        'colsample_bytree': 0.09568028958854619,\n",
    "        'min_child_weight': 3,\n",
    "        'alpha': 0.30714547261947767,\n",
    "        'random_state': 7\n",
    "    }\n",
    "    \n",
    "    \n",
    "    xgboost_regress = XGBRegressor(**xgb_space)\n",
    "    \n",
    "    xg_feature_importance = xgboost_regress.fit(X_train, y_train)\n",
    "    \n",
    "    feature_importances = normalize([xg_feature_importance.feature_importances_])[0]\n",
    "    \n",
    "    # Add in Gaussian Processs Regressor\n",
    "    dot_white_kernel = DotProduct() + WhiteKernel()\n",
    "    rat_quad_kernel = RationalQuadratic(length_scale = 1.0, alpha = 10, length_scale_bounds=(1e-06, 1000000))\n",
    "    combined_kernel = dot_white_kernel + rat_quad_kernel\n",
    "            \n",
    "    \n",
    "    gpr_dot_white = GaussianProcessRegressor(kernel = dot_white_kernel, n_restarts_optimizer = 3)\n",
    "    gpr_rat_quad = GaussianProcessRegressor(kernel = rat_quad_kernel, n_restarts_optimizer = 3)\n",
    "    gpr_combined = GaussianProcessRegressor(kernel = combined_kernel, n_restarts_optimizer = 3)\n",
    "    \n",
    "    \n",
    "    stack_regress = StackingCVRegressor(regressors=(gb_regress, xgboost_regress, lightgb_regress, gpr_dot_white,\n",
    "                                                   gpr_rat_quad, gpr_combined),\n",
    "                                meta_regressor=xgboost_regress,\n",
    "                                use_features_in_secondary=True)\n",
    "    \n",
    "    score = cv_rmse(ridge , X_train)\n",
    "    print(\"RIDGE: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n",
    "    \n",
    "    score = cv_rmse(lasso , X_train)\n",
    "    print(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n",
    "    \n",
    "    score = cv_rmse(gb_regress)\n",
    "    print(\"gb_regress: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n",
    "    \n",
    "    score = cv_rmse(lightgb_regress)\n",
    "    print(\"lightgb_regress: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n",
    "\n",
    "    score = cv_rmse(xgboost_regress)\n",
    "    print(\"xgboost_regress: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n",
    "    \n",
    "    score = cv_rmse(gpr_dot_white)\n",
    "    print(\"gpr_dot_white: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n",
    "    \n",
    "    score = cv_rmse(gpr_rat_quad)\n",
    "    print(\"gpr_rat_quad: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n",
    "    \n",
    "    score = cv_rmse(gpr_combined)\n",
    "    print(\"gpr_combined: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()) )\n",
    "    \n",
    "   \n",
    "    lasso_model = lasso.fit(X_train, y_train)\n",
    "    ridge_model = ridge.fit(X_train, y_train)\n",
    "    stack_model = stack_regress.fit(np.array(X_train), np.array(y_train))\n",
    "    gbr_model = gb_regress.fit(X_train, y_train)\n",
    "    xgb_model = xgboost_regress.fit(X_train, y_train)\n",
    "    lgb_model = lightgb_regress.fit(X_train, y_train)\n",
    "    gpr_model_dot_white = gpr_dot_white.fit(X_train, y_train)\n",
    "    gpr_model_rat_quad = gpr_rat_quad.fit(X_train, y_train)\n",
    "    gpr_model_combined = gpr_combined.fit(X_train, y_train)\n",
    "    \n",
    "    # blend and ensemble models\n",
    "    def blend_models_predict(X_train):\n",
    "        return ((0.05 * ridge_model.predict(X_train)) + \\\n",
    "                (0.1 * lasso_model.predict(X_train)) + \\\n",
    "                (0.1 * gbr_model.predict(X_train)) + \\\n",
    "                (0.15 * xgb_model.predict(X_train)) + \\\n",
    "                (0.1 * lgb_model.predict(X_train)) + \\\n",
    "                (0.05 * gpr_model_dot_white.predict(X_train)) + \\\n",
    "                (0.05 * gpr_model_rat_quad.predict(X_train)) + \\\n",
    "                (0.1 * gpr_model_combined.predict(X_train)) + \\\n",
    "                (0.3 * stack_model.predict(np.array(X_train))))\n",
    "\n",
    "    print('RMSLE score on train data:')\n",
    "    print(rmsle(y_train, blend_models_predict(X_train)))\n",
    "    \n",
    "    print('Predict submission')\n",
    "    submission = pd.read_csv(\"sample_submission.csv\")\n",
    "    submission.iloc[:,1] = (np.expm1(blend_models_predict(X_test)))\n",
    "    \n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best rmse:\n",
    "# RIDGE: 0.1134 (0.0083)\n",
    "\n",
    "# LASSO: 0.1121 (0.0075)\n",
    "\n",
    "# gb_regress: 0.1131 (0.0095)\n",
    "\n",
    "# lightgb_regress: 0.1160 (0.0105)\n",
    "\n",
    "# xgboost_regress: 0.1111 (0.0087)\n",
    "\n",
    "# gpr_dot_white: 0.1246 (0.0067)\n",
    "\n",
    "# gpr_rat_quad: 0.2060 (0.0269)\n",
    "\n",
    "# gpr_combined: 0.1280 (0.0110)\n",
    "\n",
    "# blend and ensemble models\n",
    "# 0.075 * ridge_model.predict(X_train)\n",
    "# 0.075 * lasso_model.predict(X_train)\n",
    "# 0.15 * gbr_model.predict(X_train)\n",
    "# 0.15 * xgb_model.predict(X_train)\n",
    "# 0.15 * lgb_model.predict(X_train)\n",
    "# 0.05 * gpr_model_dot_white.predict(X_train)\n",
    "# 0.05 * gpr_rat_quad.predict(X_train)\n",
    "# 0.05 * gpr_combined.predict(X_train)\n",
    "# 0.4 * stack_model.predict(np.array(X_train))\n",
    "\n",
    "# RMSLE score on train data:\n",
    "# 0.04260046872041054\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a50ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try tuning our lightgbm to optimize parametes\n",
    "if True:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "    from hyperopt.pyll import scope as ho_scope\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import KFold\n",
    "    import warnings\n",
    "\n",
    "    # Filter out LightGBM warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=Warning, message=\"bagging_fraction is set\")\n",
    "    \n",
    "    # Define the hyperparameter space\n",
    "    space = {\n",
    "        'max_depth': ho_scope.int(hp.quniform('max_depth', low=3, high=7, q=1)),\n",
    "        'learning_rate': 0.005717814490669204 # hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "        'n_estimators': 3500 # ho_scope.int(hp.quniform('n_estimators', low=1000, high=5500, q=100)), \n",
    "        'num_leaves': ho_scope.int(hp.quniform('num_leaves', low=4, high=64, q=2)),\n",
    "        'max_bin': ho_scope.int(hp.quniform('max_bin', low=10, high=250, q=10)),\n",
    "        # 'lambda_l1': hp.uniform('lambda_l1', 0, 1), # reg_alpha\n",
    "        # 'drop_rate': hp.uniform('drop_rate', 0, 1),\n",
    "        'bagging_fraction': 0.5463673909161867 # hp.uniform('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': 7 # ho_scope.int(hp.quniform('bagging_freq', low=0, high=25, q=1)),\n",
    "        'objective': 'regression',\n",
    "        'tree_learner': 'feature',\n",
    "        'boosting_type' : 'dart',\n",
    "        'xgboost_dart_mode': 'true',\n",
    "        'verbosity': -1,\n",
    "    }\n",
    "    \n",
    "    kfolds = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "    \n",
    "    # Define the objective function for regression\n",
    "    def objective(params):\n",
    "        lightgb_model = LGBMRegressor(**params)\n",
    "        # lightgb_model.fit(X_train, y_train)\n",
    "        # y_pred = lightgb_model.predict(X_test)\n",
    "        # score = np.sqrt(mean_squared_error(y_test, y_pred))  # Use MSE or another regression metric\n",
    "        score = -np.mean(cross_val_score(lightgb_model, X_train, y_train, cv=kfolds, scoring='neg_root_mean_squared_error'))\n",
    "        return {'loss': score, 'status': STATUS_OK}  # Note that loss is now the score to minimize\n",
    "    \n",
    "    # Perform the optimization\n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "    print(\"Best set of hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf915f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try tuning our xgboost first to optimize parameters\n",
    "if True:\n",
    "    from xgboost import XGBRegressor\n",
    "    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "    from hyperopt.pyll import scope as ho_scope\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Define the hyperparameter space\n",
    "    space = {\n",
    "        'max_depth': ho_scope.int(hp.quniform('max_depth', low=3, high=7, q=1)),\n",
    "        'max_leaves': ho_scope.int(hp.quniform('max_leaves', low=4, high=128, q=2)),\n",
    "        'learning_rate': 0.026278827595007454 # hp.loguniform('learning_rate', np.log(0.005), np.log(0.15)),\n",
    "        'n_estimators': 4900 # ho_scope.int(hp.quniform('n_estimators', low=1000, high=5500, q=100)),\n",
    "        # 'subsample': hp.uniform('subsample', 0, 1),\n",
    "        # 'colsample_bytree': hp.uniform('colsample_bytree', 0, 1),\n",
    "        'min_child_weight': ho_scope.int(hp.quniform('min_child_weight', low=0, high=10, q=1)),\n",
    "        'objective': 'reg:squarederror',\n",
    "        # 'alpha': hp.uniform('alpha', 0, 1),\n",
    "    }\n",
    "\n",
    "    kfolds = KFold(n_splits=10, shuffle=True, random_state=69)\n",
    "    \n",
    "    # Define the objective function for regression\n",
    "    def objective(params):\n",
    "        xgb_model = xgb.XGBRegressor(**params)\n",
    "        # xgb_model.fit(X_train, y_train)\n",
    "        # y_pred = xgb_model.predict(X_test)\n",
    "        # score = mean_squared_error(y_test, y_pred)  # Use MSE or another regression metric\n",
    "        score = -np.mean(cross_val_score(xgb_model, X_train, y_train, cv=kfolds, scoring='neg_root_mean_squared_error'))\n",
    "        return {'loss': score, 'status': STATUS_OK}  # Note that loss is now the score to minimize\n",
    "\n",
    "\n",
    "    # Perform the optimization\n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "    print(\"Best set of hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best for lightgbm\n",
    "# best loss: 0.11596685440639957\n",
    "'''\n",
    "space = {\n",
    "        'max_depth': 5,\n",
    "        'learning_rate': 0.15, \n",
    "        'n_estimators': 3800,\n",
    "        'num_leaves': 4,\n",
    "        'max_bin': 150,\n",
    "        'bagging_fraction': 0.7434141086967856,\n",
    "        'bagging_freq': 14,\n",
    "        'objective': 'regression',\n",
    "        'tree_learner': 'feature',\n",
    "        'boosting_type' : 'dart',\n",
    "        'verbosity': -1,\n",
    "    }\n",
    "'''\n",
    "\n",
    "# learning_rate = 0.11168979095966552, max_bin=192, max_depth=3, n_estimators=2100, num_leaves=91, boosting_type='dart'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best for xgboost\n",
    "# best loss: 0.11105198249739094\n",
    "'''\n",
    "space = {\n",
    "        'max_depth': 6,\n",
    "        'max_leaves': 8,\n",
    "        'learning_rate': 0.026, \n",
    "        'n_estimators': 3200, \n",
    "        'subsample': hp.uniform('subsample', 0, 1),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0, 1),\n",
    "        'min_child_weight': 7, \n",
    "        'objective': hp.choice('objective', ['reg:squarederror', 'reg:pseudohubererror']),\n",
    "        'alpha': hp.uniform('alpha', 0, 1),\n",
    "    }\n",
    "'''\n",
    "\n",
    "# learning_rate=0.008073243888388325, max_depth=3, n_estimators=5300, subsample=0.6982127263866671, colsample_bytree=0.09568028958854619, max_leaves=24, alpha=0.30714547261947767, min_child_weight=3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
